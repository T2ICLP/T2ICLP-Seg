# T2ICLP-Seg dataset
### Project Profile
  This repository is used for presentations on the T2ICLP-Seg dataset. Here, we have uploaded 50 images and 50 segmentation maps within the T2ICLP-Seg dataset, along with the corresponding text descriptions.
### FSM-Adapter
In recent years, denoising diffusion models have performed well on the task of generating images from generic texts. However, on the task of generating images from text in specific art domains, such as Chinese landscape painting, the images generated by existing methods are less effective in the evaluation dimensions such as mood expression, cultural characteristics, brush and ink techniques, due to the quantity and quality of the dataset. To impose constraints in the diffusion process and incorporate specific background information to give stronger features to the generated images, this study proposes a mask feature prediction and segmentation map called FSM-Adapter to guide the text-generated image. The method first draws on the four types of textual background information of the painting (*History, Content, Emotion*, and *Poem*), which are structurally integrated into the pre-trained Stable Diffusion model as a supplementary knowledge hierarchy to enrich the semantic information of the generated content. Secondly, the segmentation map is utilized as additional visual information to guide the generation process through the SegControl module to accurately generate layout and composition elements. Finally, the masked feature prediction module is used to output the local and global feature embedding distances between the image and the real image to achieve the detail adjustment constraints and global style constraints, thus further improving the model's ability to capture and reproduce the subtle features of Chinese landscape paintings. The synergy of the three modules enables FSM-Adapter to effectively improve the quality and expressiveness of images generated from Chinese landscape painting texts. The synergy of the three modules makes FSM-Adapter effectively improve the quality and expressiveness of text-generated images of Chinese landscape paintings. The proposed method is trained and tested on several Chinese landscape painting text-generated image datasets, and the results show that compared with previous work, the method in this paper improves the performance of CLIP-T by 1.2 percentage points and reduces the FID by 20.3 percentage points, and presents more excellent generation results in the subsequent ablation experiments and effect demonstrations. 
### code
 We will upload the training as well as the test code and the full dataset at an appropriate time!
